\subsection{Нейронные сети}\label{sec:ANN}
Задача развития искусственного интеллекта является приоритетной для Российской Федерации \cite{Russia}. Нейронные сети предназначены для задач, которые не могут быть формализованы явным образом. Примером таких задач являются задачи распознавания и классификации объектов на изображениях. Такие проблемы называют когнитивными, потому что с ними хорошо справляется человеческий мозг, но плохо справляются классические компьютерные алгоритмы.

\paragraph{Математика нейронных сетей}
Первая нейронная сеть имела архитектуру перцептрона \cite{rosenblatt1958perceptron}. Эта архитектура вдохновлена работой реальных нейронов в биологических нейронных сетях \cite{block1962perceptron}. Каждый нейрон слоя в такой сети принимает множество сигналов с предыдущего слоя, умножает их на веса, суммирует, прибавляет смещение и применяет к полученному значению нелинейную функцию (функция активации), после чего сигнал передаётся на следующий слой.
\begin{equation}\label{eq:per1}
	I_{l,i}=F\left(b_i+\sum\limits_{j}{w_{i,j}I_{l-1,j}}\right),
\end{equation}
где $F$ -- нелинейная функция активации, $w_{i,j}$ -- вес связи $i$-го и $j$-го нейрона, $b_i$ -- смещение сигнала $i$-го нейрона, $I_{l,i}$ -- сигнал $i$-го нейрона $l$-го слоя, $I_{l-1,j}$ -- сигнал $j$-го нейрона $l-1$-го слоя. Переход от слоя к слою в векторном представлении:
\begin{equation}\label{eq:per2}
	\vec{I} = F\left(\vec{J}\hat{w} + \vec{b}\right),
\end{equation}
где $\vec{I}$, $\vec{J}$ -- вектор сигналов текущего и предыдущего слоя соответственно, $\hat{w}$ - матрица весов перехода, $\vec{b}$ -- вектор смещения. По своей сути, такой переход между слоями, до применения функции активации, реализует максимальную линейную связь двух $\vec{I}$ и $\vec{J}$. Нейронная сеть типа перцептрон формируется путём совмещения множества таких слоёв разного размера. Если в такой структуре отсутствовали бы функции активации между слоями, то работа всей нейронной сети могла бы быть представлена, как совокупность линейных переходов, т.е. линейный переход. Единственный слой и так реализует максимальную линейную связь между двумя векторами и всё структуру можно было бы заменить на один слой. Поэтому, без нелинейной функции активации, увеличение количества слоёв не приводит к усложнению модели. Увеличение слоёв, при нелинейной функции активации, ведёт к увеличению количества весов нейронной сети и к её общему усложнению.
\par
Для обучения нейронной сети необходим большой обучающий набор входных векторов и функцию ошибки, которая по выходу нейронной сети вычисляет оценку соответствия выходного вектора некоторому необходимому результату. Например, если для каждого входного вектора из обучающего набора, известен желаемый выходной вектор, то функция ошибки является мерой разницы между желаемыми выходными векторами и векторами, полученным в результате обработки нейронной сетью входных векторов из обучающего набора. Введём обозначения: $\vec{x}_i$ -- $i$-ый вектор обучающего набора, $\vec{y}_i$ -- вектор, полученный в результаты обработки нейронной сетью $\vec{x}_i$, $M(\dots)$ -- функция нейронной сети -- $\vec{y}_i=M(\vec{x}_i)$, $\hat{w}$ -- линеаризованный набор параметров всех слоёв нейронной сети. Тогда функция ошибки будет иметь вид:
\begin{equation}\label{eq:loss1}
	L\left(\vec{y}_1,\vec{y}_2,\dots\right) = L\left(M(\vec{x}_1),M(\vec{x}_2),\dots\right) = L(\vec{x}_1,\vec{x}_2,\dots,\hat{w}) = L(\hat{w}).
\end{equation}
Обучающий набор постоянен, поэтому функция ошибки является функцией весов нейронной сети. Обучение заключается в минимизации функции ошибки путём варьирования параметров. Основными способами поиска глобального минимума функции ошибки являются различные модификации алгоритма градиентного спуска. Суть градиентного спуска заключается в вычислении частных производных функции ошибки по весам. Вектор этих производных является направлением, вдоль которого функция ошибки больше всего растёт. Поэтому веса изменяются в обратном направлении. На рисунке \ref{ris:SGD} отображена визуализация поверхности функции ошибки в зависимости от двух параметров и изображена траектория изменения двух весов нейронной сети \cite{amini2018spatial}.
\begin{figure}[htbp]
	\centering{\includegraphics[width=0.6\linewidth]{figures/SGD.jpg}}
	\caption{Визуализация функции ошибки в зависимости от двум весов нейронной сети и траектория изменения этих весов. Источник: \cite{amini2018spatial}.}
	\label{ris:SGD}
\end{figure}
\par
Сегодня, помимо перцептрона существует обширное множество архитектур нейронных сетей. На рисунке \ref{ris:NNArchitectures} отображено их многообразие \cite{leijnen2020neural}. Специфические архитектуры адаптированы для решения различных типов задач, в таблице \ref{tab:ArchitecturesTable} отображены характеристики и особенности некоторых архитектур.
\begin{figure}[htbp]
	\centering{\includegraphics[width=1\linewidth]{figures/NNArchitectures.png}}
	\caption{Архитектуры нейронных сетей. Источник: \cite{leijnen2020neural}.}
	\label{ris:NNArchitectures}
\end{figure}
\input{figures/ArchitecturesTable}
\par
Стоит понимать, что нейронная сеть это лишь многомерная параметризованная и очень сложная функция $\vec{y} = M(\vec{x}, \hat{w})$. При обучении подбираются её параметры, минимизирующие функцию ошибки. Чем сложнее эта функция и чем больше у неё параметров, тем выше вероятность того, что существует их комбинация, минимизирующая функцию ошибки достаточно, чтобы можно было считать, что нейронная сеть выполняет поставленную задачу.

\paragraph{Несоответствие вычислительных ресурсов}
Сегодня, нейронные сети и методы глубокого обучения являются актуальными направлениями исследований \cite{zhang2021ai}. На графике \ref{ris:PublicationsAI} показано, что в период с 2000г по 2019г, количество публикаций, связанных с искусственным интеллектом, выросло примерно в 12 раз.
\begin{figure}[h!]
	\centering{\includegraphics[width=1\linewidth]{figures/PublicationsAI.png}}
	\caption{Количество рецензированных публикаций по ИИ с 2000г по 2019г. Источник: \cite{zhang2021ai}.}
	\label{ris:PublicationsAI}
\end{figure}
На графике \ref{ris:PerfomanceGPUCPU} показано, как растёт производительность современных вычислителей со временем \cite{sun2019summarizing}. Из этой зависимости видно, что с 2015г по 2020г количество операций с $32$-х битным вещественным числом в секунду выросло примерно с $2\times10^{12}$ до $8\times10^{12}$, т.е. в 4 раза. На другом графике \ref{ris:ParametersANN} изображена зависимость параметров модели от года её публикации \cite{bernstein2021freely}. В аналогичный период с 2015г по 2020г, количество параметров выросло примерно с $5\times10^{6}$ до $1\times10^{9}$, т.е. примерно в $10^{3}$ раз. Это сравнение показывает, что количество параметров нейронных сетей, которое связанно с количеством операций необходимых для их исполнения, растёт намного быстрее, чем производительность современных вычислителей.
\begin{figure}[htbp]
	\centering{\includegraphics[width=1\linewidth]{figures/PerfomanceGPUCPU.png}}
	\caption{Скорость работы вычислительных единиц в операциях с 32|64 битным вещественным числом в секунду в зависимости от года выпуска. Источник: \cite{sun2019summarizing}.}
	\label{ris:PerfomanceGPUCPU}
\end{figure}
\begin{figure}[htbp]
	\centering{\includegraphics[width=1.0\linewidth]{figures/ParametersANN.png}}
	\caption{Количество параметров модели в зависимости от года её публикации. Источник: \cite{bernstein2021freely}.}
	\label{ris:ParametersANN}
\end{figure}
Таким образом, существует большой недостаток вычислительных мощностей, что также подтверждает компания OpenAI \cite{openai_website}, публикуя рисунок \ref{ris:OpenAI}. Для обучения больших моделей требуется создавать кластеры -- совокупности большого количества вычислителей. Компания OpenAI в 2023г сообщила, о необходимости приобретения $30000$ видеокарт Nvidia для обучения и исполнения их моделей ГПТ(Генеративный Предварительно-обученный Трансформер).
\begin{figure}[htbp]
	\centering{\includegraphics[width=1.0\linewidth]{figures/OpenAI.jpg}}
	\caption{
		Количество операций с плавающей точкой (вещественным числом) в секунду, умножить на день.
		*Один $Petaflops-days$ - вычисления со скоростью $10^{15}$ операций с плавающей точкой в секунду в течение одного дня, т.е. эта величина имеет размерность количества операций. Источник: \cite{openai_website}.}
	\label{ris:OpenAI}
\end{figure}
\par
Существует ряд проблем, сдерживающих прогресс в области вычислительной мощности, наиболее значимыми из которых являются следующие. Во первых, размер транзистора приближается к теоретическому пределу, после которого квантовые эффекты начинают играть большую роль. Были проведены расчёты минимального размера транзистора ($0.0024$нм -- длинна Комптона электрона) и года (2036г), в котором этот размер будет достигнут \cite{powell2008quantum}. Во вторых, чем плотнее транзисторы располагаются на кристалле, тем больше выделяется тепла в секунду на единицу площади процессора. Существует предел, когда создание механизма для рассеивания этого тепла становится слишком трудной задачей. На рисунке \ref{ris:WattD} отображена пиковая плотность мощности выделения тепла различных процессоров в зависимости от года их выпуска. На этом графике видно, что наибольшее значение на момент 2010 года было примерно $1000\frac{\text{мВт}}{\text{мм}^2}$, что составляет примерно $30\%$ от уровня плотности мощности тепловыделения ядерного реактора \cite{khan2021advancements}.
\begin{figure}[htbp]
	\centering{\includegraphics[width=1.0\linewidth]{figures/WattD.png}}
	\caption{Плотность тепловыделения процессоров по годам. Уровень плотности тепловыделения ядерного реактора $3100\frac{\text{мВт}}{\text{мм}^2}$. Источник: \cite{danowitz2014exploring}.}
	\label{ris:WattD}
\end{figure}
В третьих, современные вычислительные машины, построенные на архитектуре фон Неймана сталкиваются с трудностями в обеспечении ресурсоёмких вычислений с низким энергопотреблением \cite{indiveri2015memory}. Физическое разделение памяти и центрального процессора, приводит к необходимости постоянно обмениваться данными между этими частями компьютера. На подобные операции затрачивается существенное, по сравнению с обработкой информации, количество времени \cite{mahapatra1999processor}.
